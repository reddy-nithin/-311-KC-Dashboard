# ğŸ™ï¸ Kansas City 311 Insight Engine

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![BigQuery](https://img.shields.io/badge/Google-BigQuery-4285F4.svg)](https://cloud.google.com/bigquery)
[![GitHub Actions](https://img.shields.io/badge/GitHub-Actions-2088FF.svg)](https://github.com/features/actions)
[![Tableau](https://img.shields.io/badge/Tableau-Public-E97627.svg)](https://public.tableau.com/)

> **Academic Project** | Big Data Management Course  
> An automated data pipeline for analyzing Kansas City 311 service requests

---

## ğŸ“‹ Overview

This project implements an end-to-end data engineering pipeline that extracts, transforms, and loads (ETL) civic service request data from Kansas City's Open Data Portal into a cloud data warehouse for analysis and visualization.

**Key Objectives:**
- Demonstrate proficiency in cloud-based data engineering
- Automate data ingestion using CI/CD practices
- Enable real-time municipal service analytics

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        KC 311 DATA PIPELINE ARCHITECTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚   KC Open    â”‚      â”‚   Python/    â”‚      â”‚   Google     â”‚              â”‚
â”‚   â”‚   Data API   â”‚ â”€â”€â”€â–¶ â”‚   Pandas     â”‚ â”€â”€â”€â–¶ â”‚   BigQuery   â”‚              â”‚
â”‚   â”‚   (Socrata)  â”‚      â”‚   ETL        â”‚      â”‚   (DWH)      â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                               â”‚                      â”‚                       â”‚
â”‚                               â”‚                      â”‚                       â”‚
â”‚                               â–¼                      â–¼                       â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                        â”‚   GitHub     â”‚      â”‚   Tableau    â”‚               â”‚
â”‚                        â”‚   Actions    â”‚      â”‚   Public     â”‚               â”‚
â”‚                        â”‚   (Scheduler)â”‚      â”‚   (Viz)      â”‚               â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ“¸ **Architecture Diagram**: Place a visual diagram screenshot in `docs/architecture.png`

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **ğŸ”„ Automated Ingestion** | GitHub Actions runs the pipeline on a configurable schedule (weekly by default) |
| **ğŸ§¹ Data Quality** | Automated deduplication prevents duplicate records |
| **â˜ï¸ Cloud-Native** | Leverages Google BigQuery for scalable analytics |
| **ğŸ“Š Visual Analytics** | Interactive Tableau dashboard for insights |
| **ğŸ” Secure Credentials** | GCP service account keys managed via GitHub Secrets |

---

## ğŸ“Š Dashboard

<!-- TODO: Add your Tableau dashboard screenshot -->
> ğŸ“¸ **Dashboard Preview**: Place your dashboard screenshot in `docs/dashboard_preview.png`

![Dashboard Preview](docs/dashboard_preview.png)

**ğŸ”— [View Live Dashboard on Tableau Public](https://public.tableau.com/views/YOUR_DASHBOARD_LINK)**

*Replace the link above with your actual Tableau Public URL*

---

## ğŸ› ï¸ Technology Stack

| Layer | Technology |
|-------|------------|
| **Data Source** | [Kansas City Open Data](https://data.kcmo.org/) (Socrata API) |
| **Processing** | Python 3.9+, Pandas |
| **Data Warehouse** | Google BigQuery |
| **Orchestration** | GitHub Actions |
| **Visualization** | Tableau Public |

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9 or higher
- Google Cloud Platform account with BigQuery enabled
- GCP Service Account with BigQuery permissions

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/YOUR_USERNAME/311-KC-Dashboard.git
   cd 311-KC-Dashboard
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your GCP project details
   ```

5. **Add your GCP credentials**
   - Download your service account JSON key from GCP Console
   - Save it as `gbq_key.json` in the project root
   - âš ï¸ Never commit this file (it's in `.gitignore`)

### Running Locally

```bash
cd src
python pipeline.py
```

---

## ğŸ“ Project Structure

```
311-KC-Dashboard/
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example            # Environment variable template
â”œâ”€â”€ .gitignore              # Git ignore rules
â”‚
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # Configuration loader
â”‚   â””â”€â”€ pipeline.py         # Main ETL script
â”‚
â”œâ”€â”€ docs/                   # Documentation & visuals
â”‚   â”œâ”€â”€ architecture.png    # Architecture diagram
â”‚   â””â”€â”€ dashboard_preview.png
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ hourly_scheduler.yml  # GitHub Actions workflow
```

---

## âš™ï¸ GitHub Actions Configuration

The pipeline runs automatically via GitHub Actions. To configure:

1. **Add Repository Secret**
   - Go to: Settings â†’ Secrets â†’ Actions
   - Add secret named `GCP_SA_KEY` containing your service account JSON

2. **Adjust Schedule** (optional)
   - Edit `.github/workflows/hourly_scheduler.yml`
   - Modify the cron expression as needed

---

## ğŸ“ˆ Data Schema

| Column | Type | Description |
|--------|------|-------------|
| `issue_id` | STRING | Unique identifier for each request |
| `current_status` | STRING | Status (open, closed, etc.) |
| `category` | STRING | Service category |
| `open_date_time_ymd` | DATE | Request creation date |
| `resolved_date_ymd` | DATE | Resolution date (if applicable) |
| `ingest_timestamp` | INTEGER | Pipeline run timestamp |

---

## ğŸ‘¨â€ğŸ’» Author

**Nithin Reddy**  
Graduate Student | Data Engineering  
*Big Data Management Course Project*

---

## ğŸ“„ License

This project is for educational purposes as part of academic coursework.
